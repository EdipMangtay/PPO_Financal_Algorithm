# Production Pipeline - Complete Implementation

## âœ… Implementation Complete

All required modules have been implemented:

### Directory Structure

```
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ train.yaml          # Main config with HPO settings
â”‚   â”œâ”€â”€ features.yaml       # Feature engineering config
â”‚   â””â”€â”€ paths.yaml         # All file paths
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ doctor.py           # Environment validation
â”‚   â”œâ”€â”€ preflight.py        # Preflight checks + auto-fix
â”‚   â””â”€â”€ run_all_new.py      # MAIN ORCHESTRATOR (use this)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ seed.py            # Deterministic seeding
â”‚   â”œâ”€â”€ logging.py         # Structured logging
â”‚   â”œâ”€â”€ io.py              # Safe file operations
â”‚   â””â”€â”€ device.py          # GPU/CPU detection
â”œâ”€â”€ hpo/
â”‚   â””â”€â”€ optuna_search.py   # Optuna HPO per timeframe
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ trainer.py         # Safe training wrapper (OOM/NaN guards)
â”‚   â”œâ”€â”€ train_one.py       # Single timeframe training (legacy)
â”‚   â””â”€â”€ train_final.py     # Final training with best params
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ metrics.py         # Regression/classification metrics
â”œâ”€â”€ backtest/
â”‚   â”œâ”€â”€ engine.py          # Event-driven backtest
â”‚   â”œâ”€â”€ backtest.py        # Backtest on test data
â”‚   â””â”€â”€ plots.py           # Visualization
â””â”€â”€ artifacts/             # Output directory (auto-created)
    â””â”€â”€ {run_id}/
        â”œâ”€â”€ summary.json   # Global summary
        â””â”€â”€ {timeframe}/
            â”œâ”€â”€ optuna.db
            â”œâ”€â”€ optuna_best.json
            â”œâ”€â”€ model.pt
            â”œâ”€â”€ metrics_test.json
            â”œâ”€â”€ preds_test.parquet
            â”œâ”€â”€ backtest_metrics.json
            â”œâ”€â”€ trades.csv
            â”œâ”€â”€ equity.csv
            â””â”€â”€ equity_curve.png
```

## ğŸš€ Commands

### Main Command (THE ONLY ONE YOU NEED)

```bash
python scripts/run_all_new.py --config config/train.yaml --hpo_trials 50
```

### Options

```bash
# Skip HPO (use config defaults)
python scripts/run_all_new.py --config config/train.yaml --skip_hpo

# Resume existing Optuna study
python scripts/run_all_new.py --config config/train.yaml --resume_hpo

# Set HPO timeout
python scripts/run_all_new.py --config config/train.yaml --hpo_trials 50 --hpo_timeout_minutes 120

# Continue on error (don't stop if one timeframe fails)
python scripts/run_all_new.py --config config/train.yaml --continue_on_error
```

## ğŸ“‹ Pipeline Steps (Per Timeframe)

1. **Preflight Checks** â†’ Auto-fix common issues
2. **Load Data** â†’ Load and validate OHLCV data
3. **Build Features** â†’ Feature engineering with validation
4. **Split Data** â†’ Time-based split (train/val/test)
5. **Optuna HPO** â†’ Hyperparameter optimization (if enabled)
6. **Train Final Model** â†’ Train with best params on train+val
7. **Evaluate on Test** â†’ Compute metrics (MAE, RMSE, R2)
8. **Backtest on Test** â†’ Run backtest with exact metrics
9. **Save Results** â†’ All artifacts saved

## ğŸ“Š Outputs Per Timeframe

- `optuna.db` - Optuna study database
- `optuna_best.json` - Best hyperparameters
- `model.pt` - Trained model weights
- `metrics_test.json` - Test set metrics
- `preds_test.parquet` - Test predictions
- `backtest_metrics.json` - Backtest metrics
- `trades.csv` - All trades
- `equity.csv` - Equity curve
- `equity_curve.png` - Plot

## ğŸ”§ Features

âœ… **Three Independent Models** - 15m, 1h, 4h trained separately
âœ… **Optuna HPO** - Per timeframe with MedianPruner
âœ… **Preflight Auto-Fix** - Handles NaNs, duplicates, OOM
âœ… **OOM/NaN Guards** - Automatic error recovery
âœ… **Deterministic** - Same seed = same results
âœ… **No Lookahead** - Strict time-based splits
âœ… **Exact Metrics** - Precise numerical calculations
âœ… **Robust Error Handling** - No silent failures

## âš™ï¸ Configuration

Edit `config/train.yaml` to customize:
- Coin symbol
- Date range
- HPO settings (trials, timeout)
- Model architecture
- Training hyperparameters
- Backtest parameters

## ğŸ“ Deliverables Checklist

After running, verify:
- âœ… Preflight OK
- âœ… Optuna DB created per tf
- âœ… optuna_best.json created per tf
- âœ… model.pt saved per tf
- âœ… metrics_test.json saved per tf
- âœ… backtest_summary.json saved per tf
- âœ… global summary.json saved

## ğŸ¯ Next Steps

1. Install missing packages: `pip install torch pytorch-forecasting optuna pyyaml`
2. Run: `python scripts/run_all_new.py --config config/train.yaml --hpo_trials 50`
3. Check `artifacts/{run_id}/summary.json` for results


